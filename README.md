# Optimizing an ML Pipeline in Azure

## Overview
This is first of the four projects required for fullfilment of the Nanodegree **Machine Learning Engineer with Microsoft Azure**.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

You can find more information about Azure AutoML [here:](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml)

## Summary
The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y). Data consists of 20 input variables (columns) and 32,950 rows. There are 3,692 positive classes whereas 29258 negative classes.

The data used in this project can be found [here:](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv)

Detailed description of the dataset can be found [here:](https://archive.ics.uci.edu/ml/datasets/bank+marketing)

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The classification algorithm used is Logistric Regression from SKLearn framework.

**What are the benefits of the parameter sampler you chose?**
There are two arguments **C** and **max_iter**, **C** is the inverse regularization strength whereas **max_iter** is the maximum iteration to converge for the SKLearn Logistic Regression.

I have used random parameter sampling to sample over a discrete set of values. For **C**: \[1,2,3,4,5], for **max_iter**: \[80,100,120,150,170,200]
Random parameter sampling is great for discovery and getting hyperparameter combinations that you would not have guessed intuitively, although it often requires more time to execute.

**What are the benefits of the early stopping policy you chose?**
I have used [BanditPolicy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py) for early stopping. Bandit Policy defines an early termination policy based on slack criteria, and a frequency and delay interval for evaluation.

An evaluation_interval=1, slack_factor=0.2, and delay_evaluation=5 was used.

Benefits of this policy ....

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
VotingEnsemble was the best model based on the accuracy metric. The accuracy score for the model was 0.9172382397572079
Hyperparameters used for this model were 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The model generated by AutoML had accuracy slighlty higher than the HyperDrive model. 0.9167223065250379 for autoML and 0.912797167425392 for HyperDrive
The architecture is different as one is VotingEnsemble where as the other is SKLearn

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
**Improvements for hyperDrive**
Use different parameter sampling methods (Grid Parameter Sampling, Bayesian Parameter Sampling)
Use of different hyperparameter distributions (uniform, normal)
Use of different policy
Use of different primary metric (Sometimes accuracy alone doesn't represent true picture)
Increasing max total runs

**Improvements for autoML**
Change experiment timout, would allow mfor more model experimentation
Use some other primary metric
Change number of cross validations
Address class imbalance, there are 3,692 positive classes whereas 29258 negative classes
